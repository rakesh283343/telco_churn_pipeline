name: Read data
inputs:
- name: file_name
  type: String
implementation:
  container:
    image: tensorflow/tensorflow:1.13.2-py3
    command:
    - sh
    - -c
    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
      'numpy==1.17.2' 'pandas==1.0.3' 'scikit-learn==0.22.2' 'imbalanced-learn==0.6.2'
      'xgboost==1.0.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
      --no-warn-script-location 'numpy==1.17.2' 'pandas==1.0.3' 'scikit-learn==0.22.2'
      'imbalanced-learn==0.6.2' 'xgboost==1.0.2' --user) && "$0" "$@"
    - python3
    - -u
    - -c
    - |
      def read_data(file_name: str):
          import pandas as pd
          import numpy as np

          df_churn = pd.read_csv(file_name)
          print('data read complete')

          import pandas as pd
          import numpy as np

          print("one hot encoding op started")
          empty_cols = ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents',
                 'tenure', 'PhoneService', 'MultipleLines', 'InternetService',
                 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection','TechSupport',
                 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',
                 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']

          for i in empty_cols:
              df_churn[i]=df_churn[i].replace(" ",np.nan)

          df_churn.drop(['customerID'], axis=1, inplace=True)
          df_churn = df_churn.dropna()
          binary_cols = ['Partner','Dependents','PhoneService','PaperlessBilling']

          for i in binary_cols:
              df_churn[i] = df_churn[i].replace({"Yes":1,"No":0})

          #Encoding column 'gender'
          df_churn['gender'] = df_churn['gender'].replace({"Male":1,"Female":0})

          category_cols = ['PaymentMethod','MultipleLines','InternetService','OnlineSecurity',
                         'OnlineBackup','DeviceProtection',
                         'TechSupport','StreamingTV','StreamingMovies','Contract']

          for cc in category_cols:
              dummies = pd.get_dummies(df_churn[cc], drop_first=False)
              dummies = dummies.add_prefix("{}#".format(cc))
              df_churn.drop(cc, axis=1, inplace=True)
              df_churn = df_churn.join(dummies)

          df_churn['Churn'] = df_churn['Churn'].replace({"Yes":1,"No":0})
          print("Encoding Complete")
          df_churn1 = df_churn.copy(deep=True)

          import pandas as pd
          import numpy as np
          import sklearn
          from sklearn.ensemble import RandomForestClassifier
          from imblearn.over_sampling import SMOTE
          from sklearn.model_selection import train_test_split
          from sklearn.metrics import confusion_matrix
          from sklearn.model_selection import GridSearchCV
          import json

          print("op started")

          y1 = df_churn['Churn']
          X1 = df_churn.drop(['Churn'],axis=1)

          X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=0)

          #print(X_train.dtypes)
          print("tt done")
          #print(X_train.dtypes)
          #print(y_train.dtype)

          sm = SMOTE(random_state=0)
          X_train_res, y_train_res = sm.fit_sample(X_train, y_train)
          X_test_res, y_test_res = sm.fit_sample(X_test, y_test)

          param_grid = {
              'n_estimators': [50, 100, 200],
              'max_features': ['auto', 'sqrt', 'log2'],
              'max_depth' : [2,4,5,6,7,8],
              'criterion' :['gini', 'entropy']
          }

          rfc=RandomForestClassifier(random_state=42,n_estimators=100)
          gsv_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)
          rfc.fit(X_train_res, y_train_res)

          rfc_best=RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 50, max_depth=8,
                                          criterion='gini')

          rfc_best.fit(X_train, y_train) #(X_train_res, y_train_res)
          X_test_res, y_test_res = sm.fit_sample(X_test, y_test)
          y_test_pred = rfc_best.predict(X_test) #_res)
          rf_score = rfc_best.score(X_test, y_test)  #(X_test_res, y_test_res)
          conf = confusion_matrix(y_test, y_test_pred)
          print('Rf Conf:',conf)

          import pandas as pd
          import numpy as np
          from sklearn.ensemble import RandomForestClassifier
          from imblearn.over_sampling import SMOTE
          from sklearn.model_selection import GridSearchCV
          from sklearn.model_selection import train_test_split
          from sklearn.metrics import confusion_matrix
          import xgboost as xgb

          df_churn = df_churn1

          print("op started")
          y1 = df_churn['Churn']
          X1 = df_churn.drop(['Churn'],axis=1)

          X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=0)

          sm = SMOTE(random_state=0)
          X_train_res, y_train_res = sm.fit_sample(X_train, y_train)
          X_test_res, y_test_res = sm.fit_sample(X_test, y_test)

          clfxg = xgb.XGBClassifier(objective='binary:logistic', verbosity=0, max_depth=2, eta = 1, silent=0)
          clfxg.fit(X_train_res, y_train_res) #, num_round, watchlist)

          y_test_pred = clfxg.predict(X_test_res)
          conf = confusion_matrix(y_test_res, y_test_pred)
          print("XGB Conf:", conf)

      import argparse
      _parser = argparse.ArgumentParser(prog='Read data', description='')
      _parser.add_argument("--file-name", dest="file_name", type=str, required=True, default=argparse.SUPPRESS)
      _parsed_args = vars(_parser.parse_args())
      _output_files = _parsed_args.pop("_output_paths", [])

      _outputs = read_data(**_parsed_args)

      if not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):
          _outputs = [_outputs]

      _output_serializers = [

      ]

      import os
      for idx, output_file in enumerate(_output_files):
          try:
              os.makedirs(os.path.dirname(output_file))
          except OSError:
              pass
          with open(output_file, 'w') as f:
              f.write(_output_serializers[idx](_outputs[idx]))
    args:
    - --file-name
    - inputValue: file_name
